"""
Doc2MD Agent - æ ¸å¿ƒè½¬æ¢ç®¡çº¿

æµæ°´çº¿ï¼š
  1. [é¢„å¤„ç†] pandoc æå– docx â†’ ç²—ç³™ markdown + å›¾ç‰‡
  2. [AI åˆ†æ] åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜å±‚çº§ã€æ–‡æ¡£ç±»å‹ï¼‰
  3. [AI è½¬æ¢] åˆ†ç‰‡å‘é€ç»™ AIï¼Œé€ç‰‡è½¬æ¢ä¸ºä¼˜è´¨ markdown
  4. [åå¤„ç†] åˆå¹¶ç‰‡æ®µã€ç”Ÿæˆç›®å½•ã€ä¿®å¤å›¾ç‰‡è·¯å¾„ã€æ‰“åŒ…è¾“å‡º
"""

import json
import re
import logging
import shutil
from pathlib import Path
from typing import Any, Callable, Optional

from backend.llm_provider import LLMProvider
from backend.preprocessor import DocPreprocessor, split_content, fix_pandoc_table_codeblocks
from backend.prompts import (
    ANALYZE_STRUCTURE_SYSTEM, ANALYZE_STRUCTURE_USER,
    CONVERT_SYSTEM, CONVERT_USER,
    GENERATE_TOC_SYSTEM, GENERATE_TOC_USER,
)

logger = logging.getLogger(__name__)


class Doc2MDAgent:
    """æ–‡æ¡£è½¬ Markdown æ™ºèƒ½ä½“"""

    def __init__(self, config: dict, event_callback: Optional[Callable[[dict[str, Any]], None]] = None):
        self.config = config
        self.event_callback = event_callback
        self.llm = LLMProvider(config, event_callback=self._emit_event)
        self.conv_config = config.get("conversion", {})
        self.chunk_size = self.conv_config.get("chunk_size", 8000)
        self.image_dir = self.conv_config.get("image_dir", "images")
        self.generate_toc = self.conv_config.get("generate_toc", True)

    def _emit_event(self, payload: dict[str, Any]) -> None:
        if self.event_callback:
            self.event_callback(payload)

    def _report_progress(
        self,
        progress_callback: Optional[Callable[..., None]],
        stage: str,
        current: int,
        total: int,
        message: str,
    ) -> None:
        if not progress_callback:
            return
        try:
            progress_callback(stage, current, total, message)
        except TypeError:
            progress_callback(stage, current, total)

    def convert(
        self,
        input_path: str,
        output_dir: str,
        progress_callback: Optional[Callable[..., None]] = None,
    ) -> tuple[str, dict]:
        """
        å®Œæ•´è½¬æ¢æµç¨‹
        :param input_path: è¾“å…¥çš„ docx æ–‡ä»¶è·¯å¾„
        :param output_dir: è¾“å‡ºç›®å½•
        :return: (è¾“å‡ºçš„ markdown æ–‡ä»¶è·¯å¾„, token ç”¨é‡æ‘˜è¦)
        """
        input_path = Path(input_path)
        output_dir = Path(output_dir)
        work_dir = output_dir / ".work"

        output_dir.mkdir(parents=True, exist_ok=True)
        work_dir.mkdir(parents=True, exist_ok=True)
        self._emit_event(
            {
                "type": "pipeline_started",
                "message": f"å¼€å§‹å¤„ç†æ–‡æ¡£ï¼š{input_path.name}",
            }
        )

        # ========== ç¬¬ 1 æ­¥ï¼šé¢„å¤„ç† ==========
        logger.info("=" * 50)
        logger.info("ğŸ“„ ç¬¬ 1 æ­¥ï¼šæå–æ–‡æ¡£å†…å®¹å’Œå›¾ç‰‡")
        logger.info("=" * 50)
        self._report_progress(progress_callback, "preprocess", 0, 4, "é¢„å¤„ç†ä¸­ï¼šåˆå§‹åŒ–æå–å™¨")

        preprocessor = DocPreprocessor(
            input_path=str(input_path),
            work_dir=str(work_dir),
            image_dir=self.image_dir,
        )
        self._report_progress(progress_callback, "preprocess", 1, 4, "é¢„å¤„ç†ä¸­ï¼šè°ƒç”¨ pandoc æå–æ­£æ–‡ä¸å›¾ç‰‡")
        raw_md, images = preprocessor.extract()
        self._report_progress(
            progress_callback,
            "preprocess",
            2,
            4,
            f"é¢„å¤„ç†ä¸­ï¼šæå–å®Œæˆï¼Œæ­£æ–‡çº¦ {len(raw_md)} å­—ç¬¦",
        )

        # é¢„å¤„ç†ï¼šå°† pandoc å•åˆ—è¡¨æ ¼ï¼ˆå« JSON ç­‰ï¼‰è½¬ä¸ºä»£ç å—
        raw_md = fix_pandoc_table_codeblocks(raw_md)
        logger.info("å·²å®Œæˆ pandoc è¡¨æ ¼ä»£ç å—ä¿®å¤")
        self._report_progress(progress_callback, "preprocess", 3, 4, "é¢„å¤„ç†ä¸­ï¼šä¿®å¤è¡¨æ ¼ä¸­çš„ä»£ç å—")

        # æ•´ç†å›¾ç‰‡
        image_mapping = preprocessor.organize_images(output_dir, images)
        logger.info(f"å›¾ç‰‡è·¯å¾„æ˜ å°„: {image_mapping}")
        self._report_progress(
            progress_callback,
            "preprocess",
            4,
            4,
            f"é¢„å¤„ç†å®Œæˆï¼šæ•´ç†å›¾ç‰‡ {len(images)} å¼ ",
        )
        self._emit_event(
            {
                "type": "preprocess_completed",
                "image_count": len(images),
                "message": f"é¢„å¤„ç†å®Œæˆï¼šæå–æ­£æ–‡å¹¶æ•´ç†å›¾ç‰‡ {len(images)} å¼ ",
            }
        )

        # ========== ç¬¬ 2 æ­¥ï¼šAI åˆ†æç»“æ„ ==========
        logger.info("=" * 50)
        logger.info("ğŸ” ç¬¬ 2 æ­¥ï¼šAI åˆ†ææ–‡æ¡£ç»“æ„")
        logger.info("=" * 50)
        self._report_progress(progress_callback, "analyze", 0, 1, "ç»“æ„åˆ†æä¸­ï¼šå‡†å¤‡è°ƒç”¨å¤§æ¨¡å‹")

        # å–å‰ 3000 å­—ç¬¦ç»™ AI åˆ†æï¼ˆé€šå¸¸åŒ…å«æ ‡é¢˜å’Œç›®å½•ï¼‰
        analyze_content = raw_md[:3000]
        structure = self._analyze_structure(analyze_content)
        logger.info(f"æ–‡æ¡£ç±»å‹: {structure.get('doc_type', 'unknown')}")
        logger.info(f"æ ‡é¢˜æ˜ å°„: {structure.get('heading_mapping', {})}")
        self._report_progress(
            progress_callback,
            "analyze",
            1,
            1,
            f"ç»“æ„åˆ†æå®Œæˆï¼šæ–‡æ¡£ç±»å‹ {structure.get('doc_type', 'unknown')}",
        )

        # ========== ç¬¬ 3 æ­¥ï¼šAI åˆ†ç‰‡è½¬æ¢ ==========
        logger.info("=" * 50)
        logger.info("âœ¨ ç¬¬ 3 æ­¥ï¼šAI è½¬æ¢ä¸ºä¼˜è´¨ Markdown")
        logger.info("=" * 50)

        # è·³è¿‡ç›®å½•éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨æ­£æ–‡æ ‡é¢˜ä¹‹å‰ï¼‰
        content_start = self._find_content_start(raw_md)
        content_body = raw_md[content_start:]

        chunks = split_content(content_body, self.chunk_size)
        converted_chunks = []
        planned_llm_calls = 1 + len(chunks) + (1 if self.generate_toc else 0)
        self._emit_event(
            {
                "type": "llm_plan",
                "planned_calls": planned_llm_calls,
                "chunk_count": len(chunks),
                "message": f"æ­£æ–‡å·²åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µï¼Œé¢„è®¡è°ƒç”¨å¤§æ¨¡å‹ {planned_llm_calls} æ¬¡",
            }
        )

        for i, chunk in enumerate(chunks):
            logger.info(f"æ­£åœ¨è½¬æ¢ç¬¬ {i+1}/{len(chunks)} ä¸ªç‰‡æ®µ ({len(chunk)} å­—ç¬¦)...")
            self._report_progress(
                progress_callback,
                "convert",
                i,
                len(chunks),
                f"AI è½¬æ¢ä¸­ï¼šå‡†å¤‡å¤„ç†ç¬¬ {i+1}/{len(chunks)} ä¸ªåˆ†ç‰‡ï¼ˆ{len(chunk)} å­—ç¬¦ï¼‰",
            )
            converted = self._convert_chunk(
                chunk=chunk,
                structure=structure,
                chunk_index=i + 1,
                total_chunks=len(chunks),
            )
            converted_chunks.append(converted)
            self._report_progress(
                progress_callback,
                "convert",
                i + 1,
                len(chunks),
                f"AI è½¬æ¢ä¸­ï¼šå·²å®Œæˆç¬¬ {i+1}/{len(chunks)} ä¸ªåˆ†ç‰‡",
            )

        # ========== ç¬¬ 4 æ­¥ï¼šåå¤„ç† ==========
        logger.info("=" * 50)
        logger.info("ğŸ“¦ ç¬¬ 4 æ­¥ï¼šåå¤„ç†å’Œç»„è£…")
        logger.info("=" * 50)

        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        full_md = "\n\n".join(converted_chunks)

        # ä¿®å¤å›¾ç‰‡è·¯å¾„
        full_md = self._fix_image_paths(full_md, image_mapping)

        # æ¸…é™¤æ ‡é¢˜ä¸­çš„ {#xxx} é”šç‚¹å±æ€§ï¼ˆpandoc / AI æ®‹ç•™ï¼‰
        full_md = re.sub(
            r'^(#{1,6}\s+.+?)\s*\{#[^}]*\}\s*$',
            r'\1',
            full_md,
            flags=re.MULTILINE,
        )

        # ç»Ÿä¸€è¡¨æ ¼ä¸­çš„æ ‘å½¢ç¬¦å·ï¼šâ”œâ”€â”€ / â””â”€â”€ â†’ â””â”€
        full_md = full_md.replace('â”œâ”€â”€', 'â””â”€')
        full_md = full_md.replace('â””â”€â”€', 'â””â”€')

        # ç›¸é‚»çš„åŠ ç²—è¡Œä¹‹é—´åŠ ç©ºè¡Œï¼ˆé¿å…æ¸²æŸ“æˆä¸€è¡Œï¼‰
        full_md = re.sub(
            r'^(\*\*[^*]+\*\*)\n(\*\*[^*]+\*\*)$',
            r'\1\n\n\2',
            full_md,
            flags=re.MULTILINE,
        )

        # ç”Ÿæˆç›®å½•
        if self.generate_toc:
            self._report_progress(progress_callback, "toc", 0, 1, "åå¤„ç†ä¸­ï¼šç”Ÿæˆç›®å½•")
            toc = self._generate_toc(full_md)
            # åœ¨æ ‡é¢˜åæ’å…¥ç›®å½•
            full_md = self._insert_toc(full_md, toc)
            self._report_progress(progress_callback, "toc", 1, 1, "åå¤„ç†ä¸­ï¼šç›®å½•å·²æ’å…¥æ–‡æ¡£")

        # æ¸…ç† AI è¾“å‡ºä¸­å¯èƒ½æ®‹ç•™çš„ markdown ä»£ç å—æ ‡è®°
        full_md = self._clean_output(full_md)

        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        stem = input_path.stem
        output_file = output_dir / f"{stem}.md"
        output_file.write_text(full_md, encoding="utf-8")

        # æ¸…ç†å·¥ä½œç›®å½•
        shutil.rmtree(work_dir, ignore_errors=True)

        logger.info(f"âœ… è½¬æ¢å®Œæˆ: {output_file}")
        logger.info(f"   è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"   å›¾ç‰‡ç›®å½•: {output_dir / self.image_dir}")

        usage = self.llm.get_usage_summary()
        self._report_progress(progress_callback, "done", 1, 1, "è½¬æ¢å®Œæˆ")
        self._emit_event(
            {
                "type": "pipeline_completed",
                "output_file": str(output_file),
                "llm_calls": usage.get("llm_calls", 0),
                "message": f"è½¬æ¢å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{output_file.name}",
            }
        )
        return str(output_file), usage

    # ----------------------------------------------------------
    # å†…éƒ¨æ–¹æ³•
    # ----------------------------------------------------------

    def _analyze_structure(self, content: str) -> dict:
        """è°ƒç”¨ AI åˆ†ææ–‡æ¡£ç»“æ„"""
        prompt = ANALYZE_STRUCTURE_USER.format(content=content)

        try:
            response = self.llm.chat(
                ANALYZE_STRUCTURE_SYSTEM,
                prompt,
                context={"operation": "analyze_structure"},
            )
            # å»æ‰ ```json ``` åŒ…è£¹
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*', '', response)
            # æå–æœ€å¤–å±‚ JSON å¯¹è±¡
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                json_str = json_match.group()
                # å°è¯•ä¿®å¤å¸¸è§ JSON é—®é¢˜ï¼šå°¾éšé€—å·
                json_str = re.sub(r',\s*([\]}])', r'\1', json_str)
                return json.loads(json_str)
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"ç»“æ„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„: {e}")

        # é»˜è®¤ç»“æ„
        return {
            "doc_type": "api_doc",
            "heading_mapping": {},
            "has_toc": True,
            "has_json_examples": True,
        }

    def _convert_chunk(self, chunk: str, structure: dict, chunk_index: int, total_chunks: int) -> str:
        """è°ƒç”¨ AI è½¬æ¢ä¸€ä¸ªå†…å®¹ç‰‡æ®µ"""
        prompt = CONVERT_USER.format(
            structure=json.dumps(structure, ensure_ascii=False, indent=2),
            chunk_index=chunk_index,
            total_chunks=total_chunks,
            content=chunk,
        )

        response = self.llm.chat(
            CONVERT_SYSTEM,
            prompt,
            context={
                "operation": "convert_chunk",
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
            },
        )
        return response

    def _generate_toc(self, markdown: str) -> str:
        """ä»æœ€ç»ˆ markdown ä¸­æå–æ ‡é¢˜å¹¶ç”Ÿæˆç›®å½•ï¼ˆè·³è¿‡ä¸€çº§æ ‡é¢˜/æ–‡æ¡£æ ‡é¢˜ï¼‰"""
        headings = []
        for line in markdown.split("\n"):
            match = re.match(r'^(#{2,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = self._strip_heading_attrs(match.group(2).strip())
                if title == "ç›®å½•":
                    continue
                headings.append(f"{'  ' * (level - 2)}- {title}")

        if not headings:
            return ""

        headings_text = "\n".join(headings)

        try:
            prompt = GENERATE_TOC_USER.format(headings=headings_text)
            toc = self.llm.chat(
                GENERATE_TOC_SYSTEM,
                prompt,
                context={"operation": "generate_toc"},
            )
            return toc
        except Exception as e:
            logger.warning(f"AI ç›®å½•ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•ç›®å½•: {e}")
            self._emit_event(
                {
                    "type": "toc_fallback",
                    "message": f"ç›®å½•ç”Ÿæˆå¤±è´¥ï¼Œå·²åˆ‡æ¢ç®€å•ç›®å½•ç­–ç•¥ï¼š{e}",
                }
            )
            return self._simple_toc(markdown)

    def _strip_heading_attrs(self, title: str) -> str:
        """å»é™¤æ ‡é¢˜ä¸­æ®‹ç•™çš„ {#xxx} ç­‰å±æ€§"""
        return re.sub(r'\s*\{#[^}]*\}\s*$', '', title).strip()

    def _simple_toc(self, markdown: str) -> str:
        """ç®€å•çš„ç›®å½•ç”Ÿæˆï¼ˆä¸ä¾èµ– AIï¼‰ï¼Œè·³è¿‡ä¸€çº§æ ‡é¢˜å’Œç›®å½•æ ‡é¢˜"""
        toc_lines = []
        for line in markdown.split("\n"):
            match = re.match(r'^(#{2,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = self._strip_heading_attrs(match.group(2).strip())
                if title == "ç›®å½•":
                    continue
                anchor = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', title.lower())
                anchor = anchor.strip().replace(' ', '-')
                indent = "  " * (level - 2)
                toc_lines.append(f"{indent}- [{title}](#{anchor})")

        return "\n".join(toc_lines)

    def _insert_toc(self, markdown: str, toc: str) -> str:
        """åœ¨æ–‡æ¡£æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ä¿¡æ¯åã€æ­£æ–‡ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜å‰æ’å…¥ç›®å½•"""
        lines = markdown.split("\n")

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ (# xxx)
        title_pos = -1
        for i, line in enumerate(lines):
            if line.startswith("# ") and not line.startswith("## "):
                title_pos = i
                break

        if title_pos < 0:
            title_pos = 0

        # åœ¨ä¸€çº§æ ‡é¢˜ä¹‹åï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªäºŒçº§åŠä»¥ä¸‹æ ‡é¢˜ï¼ˆ## å¼€å¤´ï¼‰
        # TOC æ’å…¥åœ¨è¯¥æ ‡é¢˜ä¹‹å‰
        insert_pos = title_pos + 1
        for i in range(title_pos + 1, len(lines)):
            if re.match(r'^#{2,6}\s+', lines[i]):
                insert_pos = i
                break

        # å¦‚æœæ’å…¥ä½ç½®å‰é¢å·²æœ‰ ---ï¼Œå°±ç§»é™¤å®ƒé¿å…é‡å¤
        check_pos = insert_pos - 1
        while check_pos >= 0 and lines[check_pos].strip() == "":
            check_pos -= 1
        has_separator_before = check_pos >= 0 and lines[check_pos].strip() == "---"

        if has_separator_before:
            toc_block = f"\n## ç›®å½•\n\n{toc}\n\n---\n"
        else:
            toc_block = f"\n---\n\n## ç›®å½•\n\n{toc}\n\n---\n"
        lines.insert(insert_pos, toc_block)

        return "\n".join(lines)

    def _fix_image_paths(self, markdown: str, mapping: dict) -> str:
        """ä¿®å¤å›¾ç‰‡è·¯å¾„å¼•ç”¨"""
        result = markdown

        # å»æ‰ pandoc çš„ width/height å±æ€§
        result = re.sub(
            r'\{width="[^"]*"\s*height="[^"]*"\}',
            '',
            result
        )

        # åªåœ¨ markdown å›¾ç‰‡è¯­æ³• ![...](path) ä¸­æ›¿æ¢è·¯å¾„
        def replace_image_path(m):
            alt = m.group(1)
            path = m.group(2)

            # ç”¨æ˜ å°„è¡¨æ›¿æ¢ï¼ˆä¼˜å…ˆåŒ¹é…é•¿è·¯å¾„ï¼‰
            for old_path, new_path in sorted(mapping.items(), key=lambda x: -len(x[0])):
                if old_path in path:
                    path = path.replace(old_path, new_path)
                    break

            # é€šç”¨ä¿®å¤ï¼šmedia/media/xxx â†’ images/xxx
            path = re.sub(
                r'media/media/(\w+\.\w+)',
                lambda mm: f"{self.image_dir}/{mm.group(1)}",
                path,
            )

            # é˜²æ­¢ images/images/ åŒé‡è·¯å¾„
            while f"{self.image_dir}/{self.image_dir}/" in path:
                path = path.replace(f"{self.image_dir}/{self.image_dir}/", f"{self.image_dir}/")

            # å»æ‰ images/ ä¹‹å‰çš„å¤šä½™è·¯å¾„å‰ç¼€ï¼ˆå¦‚ output/xxx/.work/images/xxx â†’ images/xxxï¼‰
            img_dir_pos = path.find(f"{self.image_dir}/")
            if img_dir_pos > 0:
                path = path[img_dir_pos:]

            return f"![{alt}]({path})"

        result = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', replace_image_path, result)

        return result

    def _find_content_start(self, raw_md: str) -> int:
        """æ‰¾åˆ°æ­£æ–‡å¼€å§‹ä½ç½®ï¼ˆè·³è¿‡ç›®å½•åŒºåŸŸï¼‰"""
        # å¯»æ‰¾ç¬¬ä¸€ä¸ªçœŸæ­£çš„æ ‡é¢˜ï¼ˆä¸æ˜¯ç›®å½•ä¸­çš„é“¾æ¥ï¼‰
        patterns = [
            r'\n# .+\{#',     # pandoc ç”Ÿæˆçš„å¸¦é”šç‚¹æ ‡é¢˜
            r'\n# \d+',        # æ•°å­—ç¼–å·æ ‡é¢˜
            r'\n# å¼•è¨€',       # å¸¸è§çš„ä¸­æ–‡å¼€å¤´
            r'\n# Introduction',
        ]

        for pattern in patterns:
            match = re.search(pattern, raw_md)
            if match:
                return match.start()

        # fallbackï¼šè·³è¿‡å‰ 20% æˆ–æ‰¾åˆ° "---" åˆ†éš”
        return 0

    def _clean_output(self, markdown: str) -> str:
        """æ¸…ç† AI è¾“å‡º"""
        # å»æ‰ AI å¯èƒ½åŒ…è£¹çš„å¤–å±‚ ```markdown ``` æ ‡è®°
        markdown = re.sub(r'^```markdown\s*\n', '', markdown)
        markdown = re.sub(r'\n```\s*$', '', markdown)

        # åˆå¹¶è¢«åˆ†ç‰‡æˆªæ–­çš„ç›¸é‚» JSON ä»£ç å—
        markdown = self._merge_broken_json_blocks(markdown)

        # å»æ‰è¿ç»­å¤šä¸ªç©ºè¡Œ
        markdown = re.sub(r'\n{4,}', '\n\n\n', markdown)

        return markdown.strip() + "\n"

    def _merge_broken_json_blocks(self, markdown: str) -> str:
        """åˆå¹¶è¢«åˆ†ç‰‡æˆªæ–­å¯¼è‡´åˆ†è£‚çš„ç›¸é‚» JSON ä»£ç å—"""
        # åŒ¹é…: ```json ... ``` ç´§æ¥ç€ ```json ... ```ï¼ˆä¸­é—´åªæœ‰ç©ºè¡Œï¼‰
        # å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªä»£ç å—
        pattern = r'```\s*\n\s*\n*```json\s*\n'
        while re.search(pattern, markdown):
            markdown = re.sub(pattern, '\n', markdown)
        return markdown
