"""
Doc2MD Agent - æ ¸å¿ƒè½¬æ¢ç®¡çº¿

æµæ°´çº¿ï¼š
  1. [é¢„å¤„ç†] pandoc æå– docx â†’ ç²—ç³™ markdown + å›¾ç‰‡
  2. [AI åˆ†æ] åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜å±‚çº§ã€æ–‡æ¡£ç±»å‹ï¼‰
  3. [AI è½¬æ¢] åˆ†ç‰‡å‘é€ç»™ AIï¼Œé€ç‰‡è½¬æ¢ä¸ºä¼˜è´¨ markdown
  4. [åå¤„ç†] åˆå¹¶ç‰‡æ®µã€ç”Ÿæˆç›®å½•ã€ä¿®å¤å›¾ç‰‡è·¯å¾„ã€æ‰“åŒ…è¾“å‡º
"""

import json
import re
import logging
import shutil
from collections import Counter
from pathlib import Path
from typing import Any, Callable, Optional

from backend.llm_provider import LLMProvider
from backend.preprocessor import DocPreprocessor, split_content, fix_pandoc_table_codeblocks
from backend.prompts import (
    ANALYZE_STRUCTURE_SYSTEM, ANALYZE_STRUCTURE_USER,
    CONVERT_SYSTEM, CONVERT_USER,
    GENERATE_TOC_SYSTEM, GENERATE_TOC_USER,
)

logger = logging.getLogger(__name__)


class Doc2MDAgent:
    """æ–‡æ¡£è½¬ Markdown æ™ºèƒ½ä½“"""

    def __init__(self, config: dict, event_callback: Optional[Callable[[dict[str, Any]], None]] = None):
        self.config = config
        self.event_callback = event_callback
        self.llm = LLMProvider(config, event_callback=self._emit_event)
        self.conv_config = config.get("conversion", {})
        self.chunk_size = self.conv_config.get("chunk_size", 8000)
        self.image_dir = self.conv_config.get("image_dir", "images")
        self.generate_toc = self.conv_config.get("generate_toc", True)
        self.strict_mode = self.conv_config.get("strict_mode", True)
        self.chunk_strategy = self.conv_config.get("chunk_strategy", "section")
        self.max_chunk_retries = self.conv_config.get("max_chunk_retries", 2)
        self.deterministic_toc = self.conv_config.get("deterministic_toc", True)
        self.max_validation_report_items = self.conv_config.get("max_validation_report_items", 8)

    def _emit_event(self, payload: dict[str, Any]) -> None:
        if self.event_callback:
            self.event_callback(payload)

    def _report_progress(
        self,
        progress_callback: Optional[Callable[..., None]],
        stage: str,
        current: int,
        total: int,
        message: str,
    ) -> None:
        if not progress_callback:
            return
        try:
            progress_callback(stage, current, total, message)
        except TypeError:
            progress_callback(stage, current, total)

    def convert(
        self,
        input_path: str,
        output_dir: str,
        progress_callback: Optional[Callable[..., None]] = None,
    ) -> tuple[str, dict]:
        """
        å®Œæ•´è½¬æ¢æµç¨‹
        :param input_path: è¾“å…¥çš„ docx æ–‡ä»¶è·¯å¾„
        :param output_dir: è¾“å‡ºç›®å½•
        :return: (è¾“å‡ºçš„ markdown æ–‡ä»¶è·¯å¾„, token ç”¨é‡æ‘˜è¦)
        """
        input_path = Path(input_path)
        output_dir = Path(output_dir)
        work_dir = output_dir / ".work"

        output_dir.mkdir(parents=True, exist_ok=True)
        work_dir.mkdir(parents=True, exist_ok=True)
        self._emit_event(
            {
                "type": "pipeline_started",
                "message": f"å¼€å§‹å¤„ç†æ–‡æ¡£ï¼š{input_path.name}",
            }
        )

        # ========== ç¬¬ 1 æ­¥ï¼šé¢„å¤„ç† ==========
        logger.info("=" * 50)
        logger.info("ğŸ“„ ç¬¬ 1 æ­¥ï¼šæå–æ–‡æ¡£å†…å®¹å’Œå›¾ç‰‡")
        logger.info("=" * 50)
        self._report_progress(progress_callback, "preprocess", 0, 4, "é¢„å¤„ç†ä¸­ï¼šåˆå§‹åŒ–æå–å™¨")

        preprocessor = DocPreprocessor(
            input_path=str(input_path),
            work_dir=str(work_dir),
            image_dir=self.image_dir,
        )
        self._report_progress(progress_callback, "preprocess", 1, 4, "é¢„å¤„ç†ä¸­ï¼šè°ƒç”¨ pandoc æå–æ­£æ–‡ä¸å›¾ç‰‡")
        raw_md, images = preprocessor.extract()
        self._report_progress(
            progress_callback,
            "preprocess",
            2,
            4,
            f"é¢„å¤„ç†ä¸­ï¼šæå–å®Œæˆï¼Œæ­£æ–‡çº¦ {len(raw_md)} å­—ç¬¦",
        )

        # é¢„å¤„ç†ï¼šå°† pandoc å•åˆ—è¡¨æ ¼ï¼ˆå« JSON ç­‰ï¼‰è½¬ä¸ºä»£ç å—
        raw_md = fix_pandoc_table_codeblocks(raw_md)
        logger.info("å·²å®Œæˆ pandoc è¡¨æ ¼ä»£ç å—ä¿®å¤")
        self._report_progress(progress_callback, "preprocess", 3, 4, "é¢„å¤„ç†ä¸­ï¼šä¿®å¤è¡¨æ ¼ä¸­çš„ä»£ç å—")

        # æ•´ç†å›¾ç‰‡
        image_mapping = preprocessor.organize_images(output_dir, images)
        logger.info(f"å›¾ç‰‡è·¯å¾„æ˜ å°„: {image_mapping}")
        self._report_progress(
            progress_callback,
            "preprocess",
            4,
            4,
            f"é¢„å¤„ç†å®Œæˆï¼šæ•´ç†å›¾ç‰‡ {len(images)} å¼ ",
        )
        self._emit_event(
            {
                "type": "preprocess_completed",
                "image_count": len(images),
                "message": f"é¢„å¤„ç†å®Œæˆï¼šæå–æ­£æ–‡å¹¶æ•´ç†å›¾ç‰‡ {len(images)} å¼ ",
            }
        )

        # ========== ç¬¬ 2 æ­¥ï¼šç»“æ„åˆ†æï¼ˆè§„åˆ™ä¼˜å…ˆï¼‰ ==========
        logger.info("=" * 50)
        logger.info("ğŸ” ç¬¬ 2 æ­¥ï¼šåˆ†ææ–‡æ¡£ç»“æ„")
        logger.info("=" * 50)
        self._report_progress(progress_callback, "analyze", 0, 1, "ç»“æ„åˆ†æä¸­ï¼šè§„åˆ™æå–ç›®å½•ä¸ç« èŠ‚")

        expected_headings = self._extract_expected_headings_from_toc(raw_md)
        structure = self._build_rule_based_structure(raw_md, expected_headings)

        # è‹¥è§„åˆ™æå–ä¸åˆ°å¯ç”¨ç»“æ„ï¼Œå†å›é€€åˆ° AI åˆ†æ
        if not structure.get("heading_mapping"):
            logger.warning("è§„åˆ™ç»“æ„æå–å¤±è´¥ï¼Œå›é€€ AI åˆ†æ")
            analyze_content = raw_md[:3000]
            ai_structure = self._analyze_structure(analyze_content)
            structure["heading_mapping"] = ai_structure.get("heading_mapping", {})
            structure["doc_type"] = ai_structure.get("doc_type", structure.get("doc_type", "api_doc"))

        logger.info(f"æ–‡æ¡£ç±»å‹: {structure.get('doc_type', 'unknown')}")
        logger.info(f"ç›®å½•æ ‡é¢˜æ•°: {len(expected_headings)}")
        logger.info(f"æ ‡é¢˜æ˜ å°„: {structure.get('heading_mapping', {})}")
        self._report_progress(
            progress_callback,
            "analyze",
            1,
            1,
            f"ç»“æ„åˆ†æå®Œæˆï¼šæ–‡æ¡£ç±»å‹ {structure.get('doc_type', 'unknown')}",
        )

        # ========== ç¬¬ 3 æ­¥ï¼šAI åˆ†ç‰‡è½¬æ¢ ==========
        logger.info("=" * 50)
        logger.info("âœ¨ ç¬¬ 3 æ­¥ï¼šAI è½¬æ¢ä¸ºä¼˜è´¨ Markdown")
        logger.info("=" * 50)

        # è·³è¿‡ç›®å½•éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨æ­£æ–‡æ ‡é¢˜ä¹‹å‰ï¼‰
        content_start = self._find_content_start(raw_md)
        content_body = raw_md[content_start:]

        if self.chunk_strategy == "section":
            chunk_jobs = self._build_section_chunks(content_body, expected_headings)
        else:
            chunk_jobs = [
                {
                    "content": chunk,
                    "section_id": f"chunk-{idx + 1}",
                    "section_heading": "",
                    "allowed_headings": [],
                    "continuation_mode": False,
                    "chunk_has_heading": bool(re.search(r'^\s*#\s+', chunk, flags=re.MULTILINE)),
                    "previous_heading": "",
                    "next_heading": "",
                }
                for idx, chunk in enumerate(split_content(content_body, self.chunk_size))
            ]

        if not chunk_jobs:
            raise RuntimeError("æ­£æ–‡åˆ‡åˆ†å¤±è´¥ï¼šæœªç”Ÿæˆä»»ä½•åˆ†ç‰‡")

        converted_chunks = []
        planned_llm_calls = len(chunk_jobs)
        if self.generate_toc and not self.deterministic_toc:
            planned_llm_calls += 1
        self._emit_event(
            {
                "type": "llm_plan",
                "planned_calls": planned_llm_calls,
                "chunk_count": len(chunk_jobs),
                "message": f"æ­£æ–‡å·²åˆ†ä¸º {len(chunk_jobs)} ä¸ªç‰‡æ®µï¼Œé¢„è®¡è°ƒç”¨å¤§æ¨¡å‹ {planned_llm_calls} æ¬¡",
            }
        )

        for i, job in enumerate(chunk_jobs):
            chunk = job["content"]
            logger.info(
                "æ­£åœ¨è½¬æ¢ç¬¬ %s/%s ä¸ªç‰‡æ®µï¼ˆsection=%s, continuation=%s, %s å­—ç¬¦ï¼‰",
                i + 1,
                len(chunk_jobs),
                job["section_id"],
                job["continuation_mode"],
                len(chunk),
            )
            self._report_progress(
                progress_callback,
                "convert",
                i,
                len(chunk_jobs),
                f"AI è½¬æ¢ä¸­ï¼šå‡†å¤‡å¤„ç†ç¬¬ {i+1}/{len(chunk_jobs)} ä¸ªåˆ†ç‰‡ï¼ˆ{len(chunk)} å­—ç¬¦ï¼‰",
            )
            converted = self._convert_chunk_with_retry(
                chunk=chunk,
                structure=structure,
                chunk_index=i + 1,
                total_chunks=len(chunk_jobs),
                section_id=job["section_id"],
                section_heading=job["section_heading"],
                allowed_headings=job["allowed_headings"],
                continuation_mode=job["continuation_mode"],
                chunk_has_heading=job["chunk_has_heading"],
                previous_heading=job["previous_heading"],
                next_heading=job["next_heading"],
            )
            converted_chunks.append(converted)
            self._report_progress(
                progress_callback,
                "convert",
                i + 1,
                len(chunk_jobs),
                f"AI è½¬æ¢ä¸­ï¼šå·²å®Œæˆç¬¬ {i+1}/{len(chunk_jobs)} ä¸ªåˆ†ç‰‡",
            )

        # ========== ç¬¬ 4 æ­¥ï¼šåå¤„ç† ==========
        logger.info("=" * 50)
        logger.info("ğŸ“¦ ç¬¬ 4 æ­¥ï¼šåå¤„ç†å’Œç»„è£…")
        logger.info("=" * 50)

        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        full_md = "\n\n".join(converted_chunks)

        # ä¿®å¤å›¾ç‰‡è·¯å¾„
        full_md = self._fix_image_paths(full_md, image_mapping)

        # æ¸…é™¤æ ‡é¢˜ä¸­çš„ {#xxx} é”šç‚¹å±æ€§ï¼ˆpandoc / AI æ®‹ç•™ï¼‰
        full_md = re.sub(
            r'^(#{1,6}\s+.+?)\s*\{#[^}]*\}\s*$',
            r'\1',
            full_md,
            flags=re.MULTILINE,
        )

        # ç»Ÿä¸€è¡¨æ ¼ä¸­çš„æ ‘å½¢ç¬¦å·ï¼šâ”œâ”€â”€ / â””â”€â”€ â†’ â””â”€
        full_md = full_md.replace('â”œâ”€â”€', 'â””â”€')
        full_md = full_md.replace('â””â”€â”€', 'â””â”€')

        # ç›¸é‚»çš„åŠ ç²—è¡Œä¹‹é—´åŠ ç©ºè¡Œï¼ˆé¿å…æ¸²æŸ“æˆä¸€è¡Œï¼‰
        full_md = re.sub(
            r'^(\*\*[^*]+\*\*)\n(\*\*[^*]+\*\*)$',
            r'\1\n\n\2',
            full_md,
            flags=re.MULTILINE,
        )

        # ç”Ÿæˆç›®å½•
        if self.generate_toc:
            self._report_progress(progress_callback, "toc", 0, 1, "åå¤„ç†ä¸­ï¼šç”Ÿæˆç›®å½•")
            if self.deterministic_toc:
                toc = self._simple_toc(full_md)
            else:
                toc = self._generate_toc(full_md)
            # åœ¨æ ‡é¢˜åæ’å…¥ç›®å½•
            full_md = self._insert_toc(full_md, toc)
            self._report_progress(progress_callback, "toc", 1, 1, "åå¤„ç†ä¸­ï¼šç›®å½•å·²æ’å…¥æ–‡æ¡£")

        # æ¸…ç† AI è¾“å‡ºä¸­å¯èƒ½æ®‹ç•™çš„ markdown ä»£ç å—æ ‡è®°
        full_md = self._clean_output(full_md)

        if self.strict_mode:
            self._validate_final_output(raw_md=raw_md, final_md=full_md, expected_headings=expected_headings)

        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        stem = input_path.stem
        output_file = output_dir / f"{stem}.md"
        output_file.write_text(full_md, encoding="utf-8")

        # æ¸…ç†å·¥ä½œç›®å½•
        shutil.rmtree(work_dir, ignore_errors=True)

        logger.info(f"âœ… è½¬æ¢å®Œæˆ: {output_file}")
        logger.info(f"   è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"   å›¾ç‰‡ç›®å½•: {output_dir / self.image_dir}")

        usage = self.llm.get_usage_summary()
        self._report_progress(progress_callback, "done", 1, 1, "è½¬æ¢å®Œæˆ")
        self._emit_event(
            {
                "type": "pipeline_completed",
                "output_file": str(output_file),
                "llm_calls": usage.get("llm_calls", 0),
                "message": f"è½¬æ¢å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{output_file.name}",
            }
        )
        return str(output_file), usage

    # ----------------------------------------------------------
    # å†…éƒ¨æ–¹æ³•
    # ----------------------------------------------------------

    def _normalize_heading_text(self, heading: str) -> str:
        """æ ‡é¢˜æ¯”è¾ƒå½’ä¸€åŒ–ï¼šå¿½ç•¥ç©ºç™½å·®å¼‚ã€‚"""
        return re.sub(r'\s+', '', heading.strip())

    def _extract_section_id(self, numbered_heading: str) -> str:
        match = re.match(r'^(\d+(?:\.\d+)*)\s+', numbered_heading.strip())
        return match.group(1) if match else ""

    def _extract_expected_headings_from_toc(self, raw_md: str) -> list[str]:
        """ä»åŸå§‹æå–å†…å®¹ä¸­çš„ç›®å½•è¡Œæå–ç¼–å·æ ‡é¢˜åºåˆ—ã€‚"""
        headings = []
        for line in raw_md.split("\n"):
            stripped = line.strip()
            if stripped.startswith("# "):
                break
            match = re.match(r'^\[(\d+(?:\.\d+)*\s+.+?)\s+\[\d+\]\(#', stripped)
            if match:
                headings.append(match.group(1).strip())
        return headings

    def _build_rule_based_structure(self, raw_md: str, expected_headings: list[str]) -> dict[str, Any]:
        """åŸºäºç›®å½•ç¼–å·æ„å»ºç»“æ„ä¿¡æ¯ï¼Œé¿å… AI è‡ªè¡ŒçŒœæµ‹å±‚çº§ã€‚"""
        title = ""
        for line in raw_md.split("\n")[:30]:
            m = re.match(r'^\*\*(.+?)\*\*$', line.strip())
            if m and "è¯´æ˜ä¹¦" in m.group(1):
                title = m.group(1).strip()
                break

        heading_mapping: dict[str, str] = {}
        sections = []
        for heading in expected_headings:
            m = re.match(r'^(\d+(?:\.\d+)*)\s+(.+)$', heading)
            if not m:
                continue
            section_id = m.group(1)
            section_title = m.group(2).strip()
            level = min(6, len(section_id.split(".")) + 1)  # 1 -> ##, 1.1 -> ###
            heading_mapping[section_id] = "#" * level
            sections.append({"id": section_id, "title": section_title, "level": level})

        return {
            "title": title,
            "doc_type": "api_doc",
            "heading_mapping": heading_mapping,
            "has_toc": bool(expected_headings),
            "has_json_examples": True,
            "sections": sections,
        }

    def _split_raw_sections(self, content_body: str) -> list[dict[str, Any]]:
        """æŒ‰åŸå§‹ä¸€çº§æ ‡é¢˜ï¼ˆpandoc æå–åçš„ `#` è¡Œï¼‰åˆ‡åˆ†æ­£æ–‡ã€‚"""
        lines = content_body.split("\n")
        sections: list[list[str]] = []
        current: list[str] = []

        for line in lines:
            if re.match(r'^\s*#\s+', line):
                if current:
                    sections.append(current)
                current = [line]
            else:
                if not current:
                    current = [line]
                else:
                    current.append(line)

        if current:
            sections.append(current)

        result = []
        for section_lines in sections:
            content = "\n".join(section_lines)
            first_non_empty = next((ln for ln in section_lines if ln.strip()), "")
            has_heading = bool(re.match(r'^\s*#\s+', first_non_empty))
            heading_text = ""
            if has_heading:
                heading_text = re.sub(r'^\s*#\s+', '', first_non_empty).strip()
                heading_text = self._strip_heading_attrs(heading_text)
            result.append(
                {
                    "content": content,
                    "has_heading": has_heading,
                    "heading_text": heading_text,
                }
            )
        return result

    def _build_section_chunks(self, content_body: str, expected_headings: list[str]) -> list[dict[str, Any]]:
        """å…ˆæŒ‰ç« èŠ‚åˆ‡ï¼Œå†å¯¹å­ç« èŠ‚å†…è¶…é•¿å†…å®¹ç»§ç»­åˆ†ç‰‡ã€‚"""
        sections = self._split_raw_sections(content_body)
        jobs: list[dict[str, Any]] = []
        heading_index = 0

        for section in sections:
            has_heading = bool(section["has_heading"])
            numbered_heading = ""
            section_id = ""

            if has_heading:
                if heading_index < len(expected_headings):
                    numbered_heading = expected_headings[heading_index]
                else:
                    numbered_heading = section["heading_text"]
                section_id = self._extract_section_id(numbered_heading) or f"section-{heading_index + 1}"
                prev_heading = expected_headings[heading_index - 1] if heading_index > 0 else ""
                next_heading = expected_headings[heading_index + 1] if heading_index + 1 < len(expected_headings) else ""
                heading_index += 1
            else:
                section_id = f"preamble-{len(jobs) + 1}"
                prev_heading = expected_headings[heading_index - 1] if heading_index > 0 else ""
                next_heading = expected_headings[heading_index] if heading_index < len(expected_headings) else ""

            section_chunks = split_content(section["content"], self.chunk_size)
            for idx, chunk in enumerate(section_chunks):
                if not chunk.strip():
                    continue
                chunk_has_heading = bool(re.search(r'^\s*#\s+', chunk, flags=re.MULTILINE))
                jobs.append(
                    {
                        "content": chunk,
                        "section_id": section_id,
                        "section_heading": numbered_heading,
                        "allowed_headings": [numbered_heading] if numbered_heading else [],
                        "continuation_mode": idx > 0 or not chunk_has_heading,
                        "chunk_has_heading": chunk_has_heading,
                        "previous_heading": prev_heading,
                        "next_heading": next_heading,
                    }
                )

        return jobs

    def _extract_numbered_headings(self, markdown: str) -> list[str]:
        headings = []
        for line in self._remove_fenced_code_blocks(markdown).split("\n"):
            match = re.match(r'^#{2,6}\s+(.+)$', line)
            if not match:
                continue
            title = self._strip_heading_attrs(match.group(1).strip())
            if title == "ç›®å½•":
                continue
            if re.match(r'^\d', title):
                headings.append(title)
        return headings

    def _extract_error_codes(self, text: str) -> set[str]:
        """
        æå–é”™è¯¯ç ï¼ˆè¡¨æ ¼æˆ–æ™®é€šæ–‡æœ¬è¡Œï¼‰ã€‚
        ä»…ç”¨äºâ€œé”™è¯¯ç ç« èŠ‚â€å¯¹æ¯”ï¼Œé¿å…æ¨¡å‹æ‰©å†™å¤§é‡ä¸å­˜åœ¨ç¼–ç ã€‚
        """
        codes = set(re.findall(r'^\s*\|?\s*(\d{4,6})\s*(?:\||\s{2,})', text, flags=re.MULTILINE))
        return {code for code in codes if code.isdigit()}

    def _extract_json_blocks(self, text: str) -> list[str]:
        """æå– ```json fenced code block å†…å®¹ã€‚"""
        pattern = re.compile(r'```json\s*\n(.*?)\n```', re.S)
        return [m.group(1).strip() for m in pattern.finditer(text)]

    def _sanitize_json_like_text(self, text: str) -> str:
        """
        å¯¹ JSON-like æ–‡æœ¬åšè½»é‡ä¿®å¤åç”¨äºè§£æï¼š
        - å¤„ç† NBSP/è½¬ä¹‰ç¬¦
        - å»æ‰å°¾éšé€—å·
        - å°†å¸¦å­—æ¯çš„è£¸å€¼ï¼ˆå¦‚ 1118xxxx5311ï¼‰è½¬ä¸ºå­—ç¬¦ä¸²
        """
        s = text.replace("\u00a0", " ").strip()
        s = s.replace('\\"', '"')
        s = s.replace('\\[', '[')
        s = s.replace('\\]', ']')
        s = re.sub(r',\s*([}\]])', r'\1', s)

        def quote_masked_literals(m):
            prefix, value, suffix = m.group(1), m.group(2), m.group(3)
            lower = value.lower()
            if lower in {"true", "false", "null"}:
                return m.group(0)
            if re.fullmatch(r'-?\d+(?:\.\d+)?', value):
                return m.group(0)
            return f'{prefix}"{value}"{suffix}'

        s = re.sub(
            r'(:\s*)([A-Za-z0-9_./:+-]*[A-Za-z][A-Za-z0-9_./:+-]*)(\s*[,}\]])',
            quote_masked_literals,
            s,
        )
        return s

    def _normalize_json_block(self, block_text: str) -> tuple[str, bool]:
        """è¿”å› (è§„èŒƒåŒ–åçš„ JSON å­—ç¬¦ä¸², æ˜¯å¦å¯è§£æ)ã€‚"""
        candidate = self._sanitize_json_like_text(block_text)
        try:
            parsed = json.loads(candidate)
            return json.dumps(parsed, ensure_ascii=False, indent=2), True
        except Exception:
            return block_text.strip(), False

    def _replace_output_json_blocks_with_source(self, source_chunk: str, converted_chunk: str) -> str:
        """
        è‹¥æºåˆ†ç‰‡å­˜åœ¨ JSON ä»£ç å—ï¼Œåˆ™ä¼˜å…ˆå›å¡«æº JSONï¼ˆè§„èŒƒåŒ–åï¼‰åˆ°è¾“å‡ºä¸­ï¼Œ
        é¿å…æ¨¡å‹æ”¹å†™/è¡¥å†™è¿”å›ä½“ç¤ºä¾‹ã€‚
        """
        source_blocks = self._extract_json_blocks(source_chunk)
        if not source_blocks:
            return converted_chunk

        normalized_sources = []
        for block in source_blocks:
            normalized, ok = self._normalize_json_block(block)
            normalized_sources.append(normalized if ok else block.strip())

        pattern = re.compile(r'```json\s*\n(.*?)\n```', re.S)
        matches = list(pattern.finditer(converted_chunk))
        if not matches:
            appended = "\n\n".join([f"```json\n{blk}\n```" for blk in normalized_sources])
            if not converted_chunk.strip():
                return appended
            return converted_chunk.rstrip() + "\n\n" + appended

        replace_count = min(len(matches), len(normalized_sources))
        parts = []
        last_end = 0
        for idx, match in enumerate(matches):
            parts.append(converted_chunk[last_end:match.start()])
            if idx < replace_count:
                parts.append(f"```json\n{normalized_sources[idx]}\n```")
            else:
                parts.append(match.group(0))
            last_end = match.end()
        parts.append(converted_chunk[last_end:])
        if len(matches) < len(normalized_sources):
            missing = "\n\n".join(
                [f"```json\n{blk}\n```" for blk in normalized_sources[len(matches):]]
            )
            parts.append("\n\n" + missing)
        return "".join(parts)

    def _remove_fenced_code_blocks(self, text: str) -> str:
        """ç§»é™¤ fenced code blockï¼Œé¿å…æŠŠä»£ç å†…çš„ # è¯¯åˆ¤ä¸ºæ ‡é¢˜ã€‚"""
        cleaned = []
        in_code_block = False
        for line in text.split("\n"):
            if line.strip().startswith("```"):
                in_code_block = not in_code_block
                continue
            if not in_code_block:
                cleaned.append(line)
        return "\n".join(cleaned)

    def _validate_chunk_output(
        self,
        source_chunk: str,
        converted_chunk: str,
        allowed_headings: list[str],
        continuation_mode: bool,
        llm_meta: dict[str, Any],
    ) -> tuple[bool, str]:
        if llm_meta.get("truncated"):
            return False, f"æ¨¡å‹è¾“å‡ºè¢«æˆªæ–­ï¼ˆfinish_reason={llm_meta.get('finish_reason')}ï¼‰"

        output = converted_chunk.strip()
        if not output:
            return False, "æ¨¡å‹è¿”å›ç©ºå†…å®¹"

        output_no_code = self._remove_fenced_code_blocks(output)
        heading_lines = re.findall(r'^\s*#{1,6}\s+.+$', output_no_code, flags=re.MULTILINE)
        if continuation_mode and heading_lines:
            return False, "ç»­ç‰‡è¾“å‡ºåŒ…å«æ ‡é¢˜è¡Œï¼ˆcontinuation_mode=trueï¼‰"

        allowed_norm = {self._normalize_heading_text(h) for h in allowed_headings if h}
        output_numbered = self._extract_numbered_headings(output)
        output_numbered_norm = [self._normalize_heading_text(h) for h in output_numbered]

        if output_numbered_norm:
            if continuation_mode:
                return False, "ç»­ç‰‡è¾“å‡ºäº†ç¼–å·æ ‡é¢˜"
            if not allowed_norm:
                return False, f"å½“å‰ç‰‡æ®µä¸å…è®¸ç¼–å·æ ‡é¢˜ï¼Œä½†è¾“å‡ºäº† {output_numbered}"
            for heading in output_numbered_norm:
                if heading not in allowed_norm:
                    return False, f"è¾“å‡ºäº†ä¸å…è®¸çš„æ ‡é¢˜: {heading}"
            if len(output_numbered_norm) > len(allowed_norm):
                return False, "è¾“å‡ºæ ‡é¢˜æ•°é‡è¶…è¿‡å…è®¸èŒƒå›´"

        if not continuation_mode and allowed_norm and not output_numbered_norm:
            return False, "ç¼ºå°‘å¿…é¡»çš„ç¼–å·æ ‡é¢˜"

        source_json_blocks = self._extract_json_blocks(source_chunk)
        output_json_blocks = self._extract_json_blocks(output)
        if source_json_blocks:
            if len(output_json_blocks) != len(source_json_blocks):
                return False, (
                    f"JSON ä»£ç å—æ•°é‡ä¸ä¸€è‡´ï¼ˆsource={len(source_json_blocks)}, output={len(output_json_blocks)}ï¼‰"
                )
        for idx, block in enumerate(output_json_blocks, start=1):
            _, ok = self._normalize_json_block(block)
            if not ok:
                return False, f"ç¬¬ {idx} ä¸ª JSON ä»£ç å—ä¸æ˜¯åˆæ³• JSON"

        # â€œé”™è¯¯ç â€ç‰‡æ®µå¢åŠ å­é›†æ ¡éªŒï¼Œé˜²æ­¢ 100000+ å¹»è§‰æ‰©å†™
        if "é”™è¯¯ç " in source_chunk:
            source_codes = self._extract_error_codes(source_chunk)
            output_codes = self._extract_error_codes(output)
            if source_codes and output_codes and not output_codes.issubset(source_codes):
                extras = sorted(output_codes - source_codes)[:5]
                return False, f"æ£€æµ‹åˆ°è¾“å…¥ä¸­ä¸å­˜åœ¨çš„é”™è¯¯ç : {extras}"

        return True, ""

    def _convert_chunk_with_retry(
        self,
        chunk: str,
        structure: dict,
        chunk_index: int,
        total_chunks: int,
        section_id: str,
        section_heading: str,
        allowed_headings: list[str],
        continuation_mode: bool,
        chunk_has_heading: bool,
        previous_heading: str,
        next_heading: str,
    ) -> str:
        """åˆ†ç‰‡è½¬æ¢ + ä¸¥æ ¼æ ¡éªŒé‡è¯•ã€‚"""
        last_error = ""
        for attempt in range(self.max_chunk_retries + 1):
            converted, meta = self._convert_chunk(
                chunk=chunk,
                structure=structure,
                chunk_index=chunk_index,
                total_chunks=total_chunks,
                section_id=section_id,
                section_heading=section_heading,
                allowed_headings=allowed_headings,
                continuation_mode=continuation_mode,
                chunk_has_heading=chunk_has_heading,
                previous_heading=previous_heading,
                next_heading=next_heading,
                retry_reason=last_error if attempt > 0 else "",
            )
            if re.match(r'^\s*```markdown\s*\n', converted):
                converted = re.sub(r'^\s*```markdown\s*\n', '', converted)
                converted = re.sub(r'\n```\s*$', '', converted)
            converted = self._replace_output_json_blocks_with_source(chunk, converted)
            valid, reason = self._validate_chunk_output(
                source_chunk=chunk,
                converted_chunk=converted,
                allowed_headings=allowed_headings,
                continuation_mode=continuation_mode,
                llm_meta=meta,
            )
            if valid:
                return converted

            last_error = reason
            logger.warning(
                "åˆ†ç‰‡æ ¡éªŒå¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: chunk=%s/%s section=%s attempt=%s/%s reason=%s",
                chunk_index,
                total_chunks,
                section_id,
                attempt + 1,
                self.max_chunk_retries + 1,
                reason,
            )

        raise RuntimeError(
            f"åˆ†ç‰‡è½¬æ¢å¤±è´¥ï¼šç¬¬ {chunk_index}/{total_chunks} ç‰‡æ®µåœ¨ {self.max_chunk_retries + 1} æ¬¡å°è¯•åä»ä¸åˆè§„ï¼Œæœ€åé”™è¯¯ï¼š{last_error}"
        )

    def _analyze_structure(self, content: str) -> dict:
        """è°ƒç”¨ AI åˆ†ææ–‡æ¡£ç»“æ„"""
        prompt = ANALYZE_STRUCTURE_USER.format(content=content)

        try:
            response = self.llm.chat(
                ANALYZE_STRUCTURE_SYSTEM,
                prompt,
                context={"operation": "analyze_structure"},
            )
            # å»æ‰ ```json ``` åŒ…è£¹
            response = re.sub(r'```json\s*', '', response)
            response = re.sub(r'```\s*', '', response)
            # æå–æœ€å¤–å±‚ JSON å¯¹è±¡
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                json_str = json_match.group()
                # å°è¯•ä¿®å¤å¸¸è§ JSON é—®é¢˜ï¼šå°¾éšé€—å·
                json_str = re.sub(r',\s*([\]}])', r'\1', json_str)
                return json.loads(json_str)
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"ç»“æ„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æ„: {e}")

        # é»˜è®¤ç»“æ„
        return {
            "doc_type": "api_doc",
            "heading_mapping": {},
            "has_toc": True,
            "has_json_examples": True,
        }

    def _convert_chunk(
        self,
        chunk: str,
        structure: dict,
        chunk_index: int,
        total_chunks: int,
        section_id: str,
        section_heading: str,
        allowed_headings: list[str],
        continuation_mode: bool,
        chunk_has_heading: bool,
        previous_heading: str,
        next_heading: str,
        retry_reason: str = "",
    ) -> tuple[str, dict[str, Any]]:
        """è°ƒç”¨ AI è½¬æ¢ä¸€ä¸ªå†…å®¹ç‰‡æ®µï¼Œå¹¶è¿”å›å…ƒä¿¡æ¯ç”¨äºæ ¡éªŒã€‚"""
        prompt = CONVERT_USER.format(
            structure=json.dumps(structure, ensure_ascii=False, indent=2),
            section_id=section_id or "(none)",
            section_heading=section_heading or "(none)",
            continuation_mode=str(continuation_mode).lower(),
            chunk_has_heading=str(chunk_has_heading).lower(),
            allowed_headings=", ".join(allowed_headings) if allowed_headings else "(none)",
            previous_heading=previous_heading or "(none)",
            next_heading=next_heading or "(none)",
            chunk_index=chunk_index,
            total_chunks=total_chunks,
            content=chunk,
        )
        if retry_reason:
            prompt += f"\n\nä¸Šä¸€æ¬¡è¾“å‡ºä¸ç¬¦åˆçº¦æŸï¼Œå¤±è´¥åŸå› ï¼š{retry_reason}\nè¯·ä¸¥æ ¼é‡æ–°è¾“å‡ºå®Œæ•´ç‰‡æ®µã€‚"

        response = self.llm.chat_with_meta(
            CONVERT_SYSTEM,
            prompt,
            context={
                "operation": "convert_chunk",
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
                "section_id": section_id,
            },
        )
        return response.get("content", ""), response

    def _extract_error_code_sets_by_section(self, text: str) -> list[set[str]]:
        """æŒ‰â€œé”™è¯¯ç â€ç« èŠ‚é¡ºåºæå–é”™è¯¯ç é›†åˆã€‚"""
        sections = []
        current_heading = ""
        current_lines: list[str] = []

        for line in text.split("\n"):
            if re.match(r'^\s*#{1,6}\s+', line):
                if current_lines:
                    sections.append((current_heading, "\n".join(current_lines)))
                current_heading = re.sub(r'^\s*#{1,6}\s+', '', line).strip()
                current_heading = self._strip_heading_attrs(current_heading)
                current_lines = [line]
            else:
                if not current_lines:
                    current_lines = [line]
                else:
                    current_lines.append(line)

        if current_lines:
            sections.append((current_heading, "\n".join(current_lines)))

        code_sets = []
        for heading, section_text in sections:
            heading_plain = re.sub(r'^\d+(?:\.\d+)*\s+', '', heading).strip()
            if "é”™è¯¯ç " in heading_plain:
                code_sets.append(self._extract_error_codes(section_text))
        return code_sets

    def _validate_final_output(self, raw_md: str, final_md: str, expected_headings: list[str]) -> None:
        """æœ€ç»ˆè¾“å‡ºç¡¬æ ¡éªŒï¼šæ ‡é¢˜å®Œæ•´æ€§ä¸é”™è¯¯ç ä¸æ‰©å†™ã€‚"""
        issues = []

        # 1) æ ‡é¢˜åºåˆ—å®Œæ•´æ€§æ ¡éªŒ
        if expected_headings:
            expected_norm = [self._normalize_heading_text(h) for h in expected_headings]
            actual = self._extract_numbered_headings(final_md)
            actual_norm = [self._normalize_heading_text(h) for h in actual]

            expected_counter = Counter(expected_norm)
            actual_counter = Counter(actual_norm)

            missing = []
            extras = []
            for heading, count in expected_counter.items():
                diff = count - actual_counter.get(heading, 0)
                if diff > 0:
                    missing.extend([heading] * diff)
            for heading, count in actual_counter.items():
                diff = count - expected_counter.get(heading, 0)
                if diff > 0:
                    extras.extend([heading] * diff)

            if missing:
                issues.append(f"ç¼ºå¤±æ ‡é¢˜ {len(missing)} ä¸ªï¼Œä¾‹å¦‚: {missing[:self.max_validation_report_items]}")
            if extras:
                issues.append(f"æ–°å¢/é‡å¤æ ‡é¢˜ {len(extras)} ä¸ªï¼Œä¾‹å¦‚: {extras[:self.max_validation_report_items]}")

        # 2) æ–‡æ¡£ä¸»æ ‡é¢˜åªå…è®¸ 1 ä¸ª
        h1_count = len(re.findall(r'^#\s+.+$', self._remove_fenced_code_blocks(final_md), flags=re.MULTILINE))
        if h1_count > 1:
            issues.append(f"æ–‡æ¡£ä¸€çº§æ ‡é¢˜é‡å¤: {h1_count} ä¸ª")

        # 3) é”™è¯¯ç ç« èŠ‚ä¸å¾—æ‰©å†™
        raw_code_sets = self._extract_error_code_sets_by_section(raw_md)
        final_code_sets = self._extract_error_code_sets_by_section(final_md)
        for idx, final_codes in enumerate(final_code_sets):
            if idx >= len(raw_code_sets):
                if final_codes:
                    issues.append(
                        f"é”™è¯¯ç ç« èŠ‚æ•°é‡è¶…å‡ºåŸæ–‡ï¼ˆç¬¬ {idx + 1} èŠ‚ï¼‰ï¼Œæ–°å¢ä»£ç ç¤ºä¾‹: {sorted(final_codes)[:self.max_validation_report_items]}"
                    )
                continue
            raw_codes = raw_code_sets[idx]
            if raw_codes and final_codes and not final_codes.issubset(raw_codes):
                extras = sorted(final_codes - raw_codes)[:self.max_validation_report_items]
                issues.append(f"é”™è¯¯ç ç« èŠ‚ç¬¬ {idx + 1} èŠ‚å­˜åœ¨åŸæ–‡æœªå‡ºç°ç¼–ç : {extras}")

        # 4) JSON ä»£ç å—å¿…é¡»å¯è§£æï¼ˆå…è®¸æ©ç å­—æ®µåšè½»é‡ä¿®å¤åè§£æï¼‰
        invalid_json_indices = []
        for idx, block in enumerate(self._extract_json_blocks(final_md), start=1):
            _, ok = self._normalize_json_block(block)
            if not ok:
                invalid_json_indices.append(idx)
        if invalid_json_indices:
            issues.append(
                f"JSON ä»£ç å—æ ¼å¼é”™è¯¯: {invalid_json_indices[:self.max_validation_report_items]}"
            )

        if issues:
            raise RuntimeError("æœ€ç»ˆè¾“å‡ºæ ¡éªŒå¤±è´¥: " + "ï¼›".join(issues))

    def _generate_toc(self, markdown: str) -> str:
        """ä»æœ€ç»ˆ markdown ä¸­æå–æ ‡é¢˜å¹¶ç”Ÿæˆç›®å½•ï¼ˆè·³è¿‡ä¸€çº§æ ‡é¢˜/æ–‡æ¡£æ ‡é¢˜ï¼‰"""
        headings = []
        for line in markdown.split("\n"):
            match = re.match(r'^(#{2,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = self._strip_heading_attrs(match.group(2).strip())
                if title == "ç›®å½•":
                    continue
                headings.append(f"{'  ' * (level - 2)}- {title}")

        if not headings:
            return ""

        headings_text = "\n".join(headings)

        try:
            prompt = GENERATE_TOC_USER.format(headings=headings_text)
            toc = self.llm.chat(
                GENERATE_TOC_SYSTEM,
                prompt,
                context={"operation": "generate_toc"},
            )
            return toc
        except Exception as e:
            logger.warning(f"AI ç›®å½•ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•ç›®å½•: {e}")
            self._emit_event(
                {
                    "type": "toc_fallback",
                    "message": f"ç›®å½•ç”Ÿæˆå¤±è´¥ï¼Œå·²åˆ‡æ¢ç®€å•ç›®å½•ç­–ç•¥ï¼š{e}",
                }
            )
            return self._simple_toc(markdown)

    def _strip_heading_attrs(self, title: str) -> str:
        """å»é™¤æ ‡é¢˜ä¸­æ®‹ç•™çš„ {#xxx} ç­‰å±æ€§"""
        return re.sub(r'\s*\{#[^}]*\}\s*$', '', title).strip()

    def _simple_toc(self, markdown: str) -> str:
        """ç®€å•çš„ç›®å½•ç”Ÿæˆï¼ˆä¸ä¾èµ– AIï¼‰ï¼Œè·³è¿‡ä¸€çº§æ ‡é¢˜å’Œç›®å½•æ ‡é¢˜"""
        toc_lines = []
        for line in markdown.split("\n"):
            match = re.match(r'^(#{2,6})\s+(.+)$', line)
            if match:
                level = len(match.group(1))
                title = self._strip_heading_attrs(match.group(2).strip())
                if title == "ç›®å½•":
                    continue
                anchor = re.sub(r'[^\w\u4e00-\u9fff\s-]', '', title.lower())
                anchor = anchor.strip().replace(' ', '-')
                indent = "  " * (level - 2)
                toc_lines.append(f"{indent}- [{title}](#{anchor})")

        return "\n".join(toc_lines)

    def _insert_toc(self, markdown: str, toc: str) -> str:
        """åœ¨æ–‡æ¡£æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ä¿¡æ¯åã€æ­£æ–‡ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜å‰æ’å…¥ç›®å½•"""
        lines = markdown.split("\n")

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ (# xxx)
        title_pos = -1
        for i, line in enumerate(lines):
            if line.startswith("# ") and not line.startswith("## "):
                title_pos = i
                break

        if title_pos < 0:
            title_pos = 0

        # åœ¨ä¸€çº§æ ‡é¢˜ä¹‹åï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªäºŒçº§åŠä»¥ä¸‹æ ‡é¢˜ï¼ˆ## å¼€å¤´ï¼‰
        # TOC æ’å…¥åœ¨è¯¥æ ‡é¢˜ä¹‹å‰
        insert_pos = title_pos + 1
        for i in range(title_pos + 1, len(lines)):
            if re.match(r'^#{2,6}\s+', lines[i]):
                insert_pos = i
                break

        # å¦‚æœæ’å…¥ä½ç½®å‰é¢å·²æœ‰ ---ï¼Œå°±ç§»é™¤å®ƒé¿å…é‡å¤
        check_pos = insert_pos - 1
        while check_pos >= 0 and lines[check_pos].strip() == "":
            check_pos -= 1
        has_separator_before = check_pos >= 0 and lines[check_pos].strip() == "---"

        if has_separator_before:
            toc_block = f"\n## ç›®å½•\n\n{toc}\n\n---\n"
        else:
            toc_block = f"\n---\n\n## ç›®å½•\n\n{toc}\n\n---\n"
        lines.insert(insert_pos, toc_block)

        return "\n".join(lines)

    def _fix_image_paths(self, markdown: str, mapping: dict) -> str:
        """ä¿®å¤å›¾ç‰‡è·¯å¾„å¼•ç”¨"""
        result = markdown

        # å»æ‰ pandoc çš„ width/height å±æ€§
        result = re.sub(
            r'\{width="[^"]*"\s*height="[^"]*"\}',
            '',
            result
        )

        # åªåœ¨ markdown å›¾ç‰‡è¯­æ³• ![...](path) ä¸­æ›¿æ¢è·¯å¾„
        def replace_image_path(m):
            alt = m.group(1)
            path = m.group(2)

            # ç”¨æ˜ å°„è¡¨æ›¿æ¢ï¼ˆä¼˜å…ˆåŒ¹é…é•¿è·¯å¾„ï¼‰
            for old_path, new_path in sorted(mapping.items(), key=lambda x: -len(x[0])):
                if old_path in path:
                    path = path.replace(old_path, new_path)
                    break

            # é€šç”¨ä¿®å¤ï¼šmedia/media/xxx â†’ images/xxx
            path = re.sub(
                r'media/media/(\w+\.\w+)',
                lambda mm: f"{self.image_dir}/{mm.group(1)}",
                path,
            )

            # é˜²æ­¢ images/images/ åŒé‡è·¯å¾„
            while f"{self.image_dir}/{self.image_dir}/" in path:
                path = path.replace(f"{self.image_dir}/{self.image_dir}/", f"{self.image_dir}/")

            # å»æ‰ images/ ä¹‹å‰çš„å¤šä½™è·¯å¾„å‰ç¼€ï¼ˆå¦‚ output/xxx/.work/images/xxx â†’ images/xxxï¼‰
            img_dir_pos = path.find(f"{self.image_dir}/")
            if img_dir_pos > 0:
                path = path[img_dir_pos:]

            return f"![{alt}]({path})"

        result = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', replace_image_path, result)

        return result

    def _find_content_start(self, raw_md: str) -> int:
        """æ‰¾åˆ°æ­£æ–‡å¼€å§‹ä½ç½®ï¼ˆè·³è¿‡ç›®å½•åŒºåŸŸï¼‰"""
        # å¯»æ‰¾ç¬¬ä¸€ä¸ªçœŸæ­£çš„æ ‡é¢˜ï¼ˆä¸æ˜¯ç›®å½•ä¸­çš„é“¾æ¥ï¼‰
        patterns = [
            r'\n# .+\{#',     # pandoc ç”Ÿæˆçš„å¸¦é”šç‚¹æ ‡é¢˜
            r'\n# \d+',        # æ•°å­—ç¼–å·æ ‡é¢˜
            r'\n# å¼•è¨€',       # å¸¸è§çš„ä¸­æ–‡å¼€å¤´
            r'\n# Introduction',
        ]

        for pattern in patterns:
            match = re.search(pattern, raw_md)
            if match:
                return match.start()

        # fallbackï¼šè·³è¿‡å‰ 20% æˆ–æ‰¾åˆ° "---" åˆ†éš”
        return 0

    def _clean_output(self, markdown: str) -> str:
        """æ¸…ç† AI è¾“å‡º"""
        # å»æ‰ AI å¯èƒ½åŒ…è£¹çš„å¤–å±‚ ```markdown ``` æ ‡è®°
        markdown = re.sub(r'^```markdown\s*\n', '', markdown)
        markdown = re.sub(r'\n```\s*$', '', markdown)

        # åˆå¹¶è¢«åˆ†ç‰‡æˆªæ–­çš„ç›¸é‚» JSON ä»£ç å—
        markdown = self._merge_broken_json_blocks(markdown)

        # å»æ‰è¿ç»­å¤šä¸ªç©ºè¡Œ
        markdown = re.sub(r'\n{4,}', '\n\n\n', markdown)

        return markdown.strip() + "\n"

    def _merge_broken_json_blocks(self, markdown: str) -> str:
        """åˆå¹¶è¢«åˆ†ç‰‡æˆªæ–­å¯¼è‡´åˆ†è£‚çš„ç›¸é‚» JSON ä»£ç å—"""
        # åŒ¹é…: ```json ... ``` ç´§æ¥ç€ ```json ... ```ï¼ˆä¸­é—´åªæœ‰ç©ºè¡Œï¼‰
        # å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªä»£ç å—
        pattern = r'```\s*\n\s*\n*```json\s*\n'
        while re.search(pattern, markdown):
            markdown = re.sub(pattern, '\n', markdown)
        return markdown
